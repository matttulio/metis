{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Scalability Analysis\n",
    "\n",
    "Testing scalability properties across four neural network architectures:\n",
    "\n",
    "1. **ReLU-based Neural Networks**\n",
    "    - Neural networks with nonlinearities parametrized by ReLU functions\n",
    "\n",
    "2. **B-Spline Neural Networks** \n",
    "    - Neural networks with nonlinearities parametrized by B-Splines\n",
    "\n",
    "3. **Quasi-Ground Truth Neural Networks**\n",
    "    - Neural networks with true nonlinearities and learnable parameters \n",
    "\n",
    "4. **Ground Truth Networks**\n",
    "    - Neural networks with true nonlinearities and true parameters\n",
    "\n",
    "This comparison will help evaluate performance and computational efficiency across different architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable\n",
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, random, config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LearnableActivations` class is a custom neural network module designed to apply multiple learnable activation functions to input data. The key heuristic behind this class is to enable flexibility and adaptability in activation functions by parameterizing them with learnable parameters. \n",
    "\n",
    "#### Key Features:\n",
    "1. **Input Splitting**: The input features are divided into equal parts, corresponding to the number of activation functions provided. This ensures each activation function operates on a distinct subset of the input.\n",
    "\n",
    "2. **Learnable Parameters**: Each activation function is associated with a fixed number of learnable parameters (default is 2). These parameters are initialized and stored in a single array.\n",
    "\n",
    "3. **Iterative Activation**: The module applies each activation function to its respective input split, concatenating the results to produce the final output.\n",
    "\n",
    "This approach allows for dynamic and trainable activation functions, making the model more expressive and capable of learning complex patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnableActivations(nn.Module):\n",
    "    input_features: int\n",
    "    activations: List[Callable]\n",
    "    n_params: int = 2\n",
    "\n",
    "    def setup(self):\n",
    "        self.num_activations = len(self.activations)\n",
    "        if self.input_features % self.num_activations != 0:\n",
    "            raise ValueError(\"Input features must be divisible by num_activations\")\n",
    "\n",
    "        # All params stored in a single array: shape (num_activations, n_params)\n",
    "        self.params = self.param(\n",
    "            \"activation_params\",\n",
    "            nn.initializers.ones,\n",
    "            (self.num_activations, self.n_params),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        splits = jnp.split(x, self.num_activations, axis=-1)\n",
    "\n",
    "        # Initialize with first activation\n",
    "        activated = self.activations[0](splits[0], self.params[0])\n",
    "\n",
    "        # Iteratively concatenate remaining activations\n",
    "        for i in range(1, self.num_activations):\n",
    "            activated = jnp.concatenate(\n",
    "                [activated, self.activations[i](splits[i], self.params[i])], axis=-1\n",
    "            )\n",
    "\n",
    "        return activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedActivations(nn.Module):\n",
    "    input_features: int\n",
    "    activations: List[Callable]\n",
    "\n",
    "    def setup(self):\n",
    "        self.num_activations = len(self.activations)\n",
    "        if self.input_features % self.num_activations != 0:\n",
    "            raise ValueError(\"Input features must be divisible by num_activations\")\n",
    "\n",
    "    def __call__(self, x):\n",
    "        splits = jnp.split(x, self.num_activations, axis=-1)\n",
    "\n",
    "        # Initialize with first activation\n",
    "        activated = self.activations[0](splits[0])\n",
    "\n",
    "        # Iteratively concatenate remaining activations\n",
    "        for i in range(1, self.num_activations):\n",
    "            activated = jnp.concatenate(\n",
    "                [activated, self.activations[i](splits[i])], axis=-1\n",
    "            )\n",
    "\n",
    "        return activated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    input_dim: int  # Neurons per activation group\n",
    "    n_funcs: int  # Number of activation functions\n",
    "    output_dim: int\n",
    "    activations: List[Callable]\n",
    "    max_num_params: int = 2\n",
    "\n",
    "    def setup(self):\n",
    "        self.custom_activation = LearnableActivations(\n",
    "            self.input_dim * self.n_funcs,\n",
    "            self.activations,\n",
    "            self.max_num_params,\n",
    "        )\n",
    "        self.output_layer = nn.Dense(self.output_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.custom_activation(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedModel(nn.Module):\n",
    "    input_dim: int  # Neurons per activation group\n",
    "    n_funcs: int  # Number of activation functions\n",
    "    output_dim: int\n",
    "    activations: List[Callable]\n",
    "\n",
    "    def setup(self):\n",
    "        self.custom_activation = FixedActivations(\n",
    "            self.input_dim * self.n_funcs,\n",
    "            self.activations,\n",
    "        )\n",
    "        self.output_layer = nn.Dense(self.output_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.custom_activation(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[0.         1.57079633 1.57079633 0.         1.57079633 1.57079633\n",
      "  0.         1.57079633 1.57079633]\n",
      " [0.         1.57079633 1.57079633 0.         1.57079633 1.57079633\n",
      "  0.         1.57079633 1.57079633]\n",
      " [0.         1.57079633 1.57079633 0.         1.57079633 1.57079633\n",
      "  0.         1.57079633 1.57079633]\n",
      " [0.         1.57079633 1.57079633 0.         1.57079633 1.57079633\n",
      "  0.         1.57079633 1.57079633]\n",
      " [0.         1.57079633 1.57079633 0.         1.57079633 1.57079633\n",
      "  0.         1.57079633 1.57079633]]\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def func(x, params):\n",
    "    return (\n",
    "        params[0] * nn.relu(x + params[1])\n",
    "        + params[2] * nn.relu(x + params[3])\n",
    "        + params[4] * nn.relu(x + params[5])\n",
    "    )\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "# def func1(x, params):\n",
    "#     return x * params[0]\n",
    "\n",
    "\n",
    "@jit\n",
    "def func2(x):\n",
    "    return 0.5 + x * x * -1\n",
    "\n",
    "\n",
    "# activations = [func1, func2, func]\n",
    "activations = [func2] * 3\n",
    "# activations = [func, func, func]\n",
    "\n",
    "L = 3\n",
    "input_dim = 3\n",
    "N = input_dim\n",
    "output_dim = 3\n",
    "batch_size = 5\n",
    "\n",
    "model = FixedModel(N, L, output_dim=output_dim, activations=activations)\n",
    "key = random.key(0)\n",
    "x1 = jnp.zeros((batch_size, 1))\n",
    "x2 = jnp.ones((batch_size, 1)) * jnp.pi / 2\n",
    "x3 = jnp.ones((batch_size, 1)) * jnp.pi / 2\n",
    "x = jnp.concatenate([x1, x2, x3], axis=1)\n",
    "\n",
    "x = jnp.tile(x, L)\n",
    "\n",
    "params = model.init(key, x)\n",
    "\n",
    "output = model.apply(params, x)\n",
    "# print(\"Model parameters:\")\n",
    "# print(params[\"params\"][\"output_layer\"][\"kernel\"])\n",
    "# print(jnp.sum(params[\"params\"][\"output_layer\"][\"kernel\"], axis=0))\n",
    "# print(\"Input shape:\", x.shape)\n",
    "# print(\"Hidden layer output shape:\", model.hidden_layer(x).shape)\n",
    "# print(\"After activation shape:\", model.activation(model.hidden_layer(x)).shape)\n",
    "# print(\"Final output shape:\", output.shape)\n",
    "print(\"Input:\", x)\n",
    "print(output.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
