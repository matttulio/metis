{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from optax import adam\n",
    "from src.datagen import Equations\n",
    "import os\n",
    "from jax import jit, vmap, random, lax\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import scienceplots\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "plt.style.use(\"science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_variables = 20\n",
    "expected_number_of_nls = 10\n",
    "input_dim = state_variables * expected_number_of_nls\n",
    "output_dim = state_variables\n",
    "\n",
    "model = ZeroLayersNN(N=input_dim, L=expected_number_of_nls, output_dim=state_variables)\n",
    "seed = 42\n",
    "train_batch_size = 32\n",
    "test_batch_size = 64\n",
    "\n",
    "\n",
    "dummy_input = jnp.zeros((train_batch_size, input_dim))\n",
    "key = random.key(seed)\n",
    "key, subkey = random.split(key)\n",
    "params = model.init(subkey, dummy_input)\n",
    "optimizer = adam(learning_rate=0.001)\n",
    "state = TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
    "print(\n",
    "    f\"There are {count_params(state.params['params'])} learnable parameters in the current architecture.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alphabeta(dim, num, spread, seed):\n",
    "    key = random.key(seed)\n",
    "    shape = (dim, num)\n",
    "    A = random.normal(key, shape)\n",
    "    # Q, _, _ = jnp.linalg.svd(A, full_matrices=True)\n",
    "    return A * spread\n",
    "\n",
    "\n",
    "def generate_callable_functions(dim, num, spread, seed):\n",
    "    bases = generate_alphabeta(dim, num, spread, seed)  # Get num basis vectors\n",
    "    bases = bases.T\n",
    "    alphas = bases[:, : dim // 2].T  # First half of Q are alphas\n",
    "    gammas = bases[:, dim // 2 :].T\n",
    "\n",
    "    def make_function(alpha, gamma):\n",
    "        @jit\n",
    "        def func(x):\n",
    "            return (\n",
    "                alpha[0] * nn.relu(x + gamma[0])\n",
    "                + alpha[1] * nn.relu(x + gamma[1])\n",
    "                + alpha[2] * nn.relu(x + gamma[2])\n",
    "            )\n",
    "\n",
    "        return func\n",
    "\n",
    "    return tuple([make_function(alphas[i], gammas[i]) for i in range(num)]), bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = 6\n",
    "n_nls = 10\n",
    "spread = 5\n",
    "alphagamma = generate_alphabeta(dims, n_nls, spread, seed)\n",
    "alphagamma = alphagamma.T\n",
    "alphas = alphagamma[:, : dims // 2].T  # First half of Q are alphas\n",
    "gammas = alphagamma[:, dims // 2 :].T\n",
    "print(alphagamma.shape)\n",
    "print(\"Alphas:\", alphas)\n",
    "print(\"Gammas:\", gammas)\n",
    "\n",
    "non_lins, vec_nls = generate_callable_functions(dims, n_nls, spread, seed)\n",
    "\n",
    "config = {\n",
    "    \"n_vars\": state_variables,\n",
    "    \"n_eqs\": state_variables,\n",
    "    \"bounds_addends\": (1, 3),\n",
    "    \"bounds_multiplicands\": (1, 1),\n",
    "    \"non_lins\": non_lins,\n",
    "    \"sym_non_lins\": None,\n",
    "    \"distribution\": \"uniform\",\n",
    "    \"a\": None,\n",
    "    \"b\": None,\n",
    "    \"sigma\": None,\n",
    "    \"p\": None,\n",
    "    \"seed\": seed,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for individual non-linearities\n",
    "num_funcs = len(non_lins)\n",
    "cols = 2  # Number of columns in the grid\n",
    "rows = (num_funcs + 1) // cols  # Rows for individual plots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(12, 18), sharex=True)\n",
    "\n",
    "x = jnp.linspace(-10, 10, 1000)\n",
    "colors = plt.cm.tab10(range(num_funcs))  # Use a colormap to assign colors\n",
    "\n",
    "# Plot each callable's result in separate subplots\n",
    "for i, func in enumerate(non_lins):\n",
    "    row, col = divmod(i, cols)\n",
    "    ax = axes[row, col]\n",
    "    y = func(x)\n",
    "    ax.plot(x, y, label=r\"$\\text{nl}_{%d}$\" % i, color=colors[i])\n",
    "    ax.set_xlabel(\"x\")\n",
    "    ax.set_ylabel(\"y\")\n",
    "    ax.set_title(f\"Non-Linearity {i}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Adjust layout for individual plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()\n",
    "\n",
    "# Create a separate figure for all non-linearities together\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, func in enumerate(non_lins):\n",
    "    y = func(x)\n",
    "    plt.plot(x, y, label=r\"$\\text{nl}_{%d}$\" % i, color=colors[i])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"All Non-Linearities Together\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = Equations(config)\n",
    "system.save_symb_expr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_eval(v):\n",
    "    return system(y=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bound = 15\n",
    "train_size = int(1e4)\n",
    "test_ratio = 0.2\n",
    "test_size = int(test_ratio * train_size)\n",
    "total_size = train_size + test_size\n",
    "train_values = jnp.tile(\n",
    "    jnp.linspace(-1 * bound, bound, total_size).reshape(-1, 1), (1, config[\"n_eqs\"])\n",
    ")\n",
    "\n",
    "# Randomly select `test_size` elements for inbound_test\n",
    "key = random.key(seed)\n",
    "key, subkey = random.split(key)  # Ensure reproducibility\n",
    "indices = random.choice(\n",
    "    subkey, total_size, shape=(test_size,), replace=False\n",
    ")  # Unique indices\n",
    "inbound_test = train_values[indices]  # Extract test points\n",
    "mask = jnp.ones(total_size, dtype=bool)\n",
    "mask = mask.at[indices].set(False)  # Mask out test points\n",
    "train_values = train_values[mask]  # Remove test points from training set\n",
    "\n",
    "# Create out-of-bound test values\n",
    "range1 = jnp.linspace(-1 * bound - 5, -1 * bound, test_size // 2)\n",
    "range2 = jnp.linspace(bound, bound + 5, test_size // 2)\n",
    "outofbound_test = jnp.concatenate((range1, range2), axis=0).reshape(-1, 1)\n",
    "outofbound_test = jnp.tile(outofbound_test, (1, config[\"n_eqs\"]))\n",
    "\n",
    "# Stack both test sets\n",
    "stacked_test = jnp.vstack((inbound_test, outofbound_test))\n",
    "\n",
    "# Concatenate train and test values\n",
    "values = jnp.vstack((train_values, stacked_test))\n",
    "\n",
    "# Standardize values\n",
    "values_mean = jnp.mean(values, axis=0)\n",
    "values_std = jnp.std(values, axis=0)\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "evaluated_values = vmap(my_eval)(values)\n",
    "values = (values - values_mean) / values_std\n",
    "\n",
    "# Standardize evaluated_values\n",
    "evaluated_values_mean = jnp.mean(evaluated_values, axis=0)\n",
    "evaluated_values_std = jnp.std(evaluated_values, axis=0)\n",
    "evaluated_values = (evaluated_values - evaluated_values_mean) / evaluated_values_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(config[\"n_eqs\"]):\n",
    "    plt.scatter(values[:, 0], evaluated_values[:, i], label=f\"Equation {i+1}\", s=0.1)\n",
    "plt.xlabel(\"Input Values\")\n",
    "plt.ylabel(\"Evaluated Values\")\n",
    "plt.title(\"Evaluation of the System\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_values = jnp.repeat(values, expected_number_of_nls, axis=1)\n",
    "train_values = expanded_values[:train_size]\n",
    "inbound_test = expanded_values[train_size : test_size + train_size]\n",
    "outofbound_test = expanded_values[test_size + train_size :]\n",
    "\n",
    "train_outputs, inbound_test_outputs = (\n",
    "    evaluated_values[:train_size],\n",
    "    evaluated_values[train_size : test_size + train_size],\n",
    ")\n",
    "outofbound_test_outputs = evaluated_values[test_size + train_size :]\n",
    "\n",
    "train_batches = create_batches(train_values, train_outputs, train_batch_size)\n",
    "inbound_test_batches = create_batches(\n",
    "    inbound_test, inbound_test_outputs, test_batch_size\n",
    ")\n",
    "outofbound_test_batches = create_batches(\n",
    "    outofbound_test, outofbound_test_outputs, test_batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = jnp.concatenate(\n",
    "    (\n",
    "        state.params[\"params\"][\"CustomActivation_0\"][\"alpha\"],\n",
    "        state.params[\"params\"][\"CustomActivation_0\"][\"gamma\"],\n",
    "    ),\n",
    "    axis=0,\n",
    ").T\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=(1))\n",
    "def loss_fn(params, apply_fn, batch_x, batch_y, l1_lambda=0.01):\n",
    "    predictions = apply_fn(params, batch_x)\n",
    "    return jnp.mean((predictions - batch_y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_functions(loss_fn):\n",
    "    @jit\n",
    "    def train_epoch(carry, epoch):\n",
    "        state, _ = carry  # Unpack state and dummy loss accumulator\n",
    "        epoch_loss = jnp.array(0.0)\n",
    "\n",
    "        # Loop over batches\n",
    "        def batch_step(carry, batch):\n",
    "            state, _ = carry\n",
    "            batch_x, batch_y = batch\n",
    "            state, train_loss = train_step(state, batch_x, batch_y, loss_fn)\n",
    "            inbound_test_loss = evaluate(state, inbound_test_batches, loss_fn)\n",
    "            outofbound_test_loss = evaluate(state, outofbound_test_batches, loss_fn)\n",
    "\n",
    "            return (state, train_loss), (\n",
    "                train_loss,\n",
    "                inbound_test_loss,\n",
    "                outofbound_test_loss,\n",
    "            )\n",
    "\n",
    "        (state, _), (\n",
    "            train_losses,\n",
    "            inbound_test_losses,\n",
    "            outofbound_test_losses,\n",
    "        ) = lax.scan(batch_step, (state, 0.0), train_batches)\n",
    "\n",
    "        # Print every n epochs\n",
    "        print_condition = epoch % 5 == 0\n",
    "        jax.lax.cond(\n",
    "            print_condition,\n",
    "            # True branch: print metrics\n",
    "            lambda: jax.debug.print(\n",
    "                \"Epoch: {epoch}, Train Loss: {train_loss:.4e}, Inbound-Test Loss: {inbound_test_loss:.4e}, Outofbound-Test Loss: {outofbound_test_loss:.4e} \",\n",
    "                epoch=epoch,\n",
    "                train_loss=jnp.mean(train_losses),\n",
    "                inbound_test_loss=jnp.mean(inbound_test_losses),\n",
    "                outofbound_test_loss=jnp.mean(outofbound_test_losses),\n",
    "            ),\n",
    "            # False branch: do nothing\n",
    "            lambda: None,\n",
    "        )\n",
    "        return (state, epoch_loss), (\n",
    "            train_losses,\n",
    "            inbound_test_losses,\n",
    "            outofbound_test_losses,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step, evaluate, train_epoch = make_training_functions(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 201\n",
    "\n",
    "(state, loss), (train_losses, inbound_test_losses, outofbound_test_losses) = lax.scan(\n",
    "    train_epoch, (state, 0.0), jnp.arange(num_epochs)\n",
    ")\n",
    "jax.block_until_ready(state)\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
